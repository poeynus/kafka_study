# 카프카 공부 정리

## 카프카란?

***메시징 시스템***

비동기 통신 프로토콜을 지닌 즉각적인 응답이 필요 하지 않은 큐 시스템

메시지 큐는 서로 독립적으로 분리되어 처리

AMQP(Advanced Message Queuing Protocol) : 표준화된 메시징 프로토콜(TCP 5672), 신뢰성이 있는 메시지

> 카카오 구매하기, 구매내역은 RabbitMQ를 사용해서 서비스 하고 있음

***Pub Sub모델***

Publish, Subscribe, 메시지에 특정한 수신자가 정해져 있지 않다. 구독을 신청한 수신자에게 전달

> LinkedIn은 초창기에 ActiveMQ를 사용하여 구현했었는데 서비스가 커지고 불편하여서 kafka를 직접 개발 함

***카프카의 탄생***

기존에 느끼던 불편함을 해소하기 위해 개발, 복잡했던 구조가 카프카를 중간에 두니 간편해짐

> 1. 높은 처리량 
> 2. 실시간 로그 통합 
> 3. 무중단 
> 4. 이기종과의 호환성 
> 5. 스케일 아웃 
> 6. 프로듀서와 컨슈머 분리

***성능 비교***

ThroughPut : Kafka > Pulsar > RabbitMQ

End-To-End Latency Quantiles: Kafka > Pulsar > RabbitMQ

***컨플루언트**

카프카를 오픈소스로 등록 후, 타 회사에서 구성 요청이 왔는데 링크드인에서 발생했던 문제들이 똑같이 발생하는 것을 보고 수익성을 발견해 창립한 회사

***카프카***

오픈 소스 분산 이벤트 스트리밍 플랫폼이며, 쓰기에 최적화 된 플랫폼이다.

***사용 사례 및 사용 이유***

예상치 못한 대량의 요청이 올 경우 장애가 발생하게 된다. 카프카를 사용하면 비동기 이기에 이떄 보관하고 있다가 복구 될 시 안정적으로 처리할 수 있다.

> 1. 무향 로션 검색 
> 2. 무향 로션 검색 로그 카프카로 전달 (프로듀서)
> 3. 카프카에서 분석 시스템 내용 전달 (컨슈머)
> 4. 분석 결과 도출
> 5. 분석 결과 반영

> 적어도 한번 전송 방식: 중복은 괜찮은데 데이터가 유실 되면 안된다.

## 카프카 기본 구성

사진

프로듀서: 카프카로 데이터를 전송해주는 것

컨슈머: 카프카로 들어온 메시지를 꺼내서 사용하는 것 

> 예전에는 consumer offest을 주키퍼 저장소에 저장했었기 때문에 주키퍼와 컨슈머가 연결이 되어 있어야 했었다.

***카프카 클러스터**

주키퍼는 quorum mode로 인해 반드시 홀수로 구성해야 하고 브로커는 제한이 없다. 

주키퍼 3대, 브로커 3대가 기본이다.

## 주키퍼 

코디네이션 애플리케이션이다. 카프카의 메타 데이터를 관리한다. KIP-500이라고 주키퍼 제거를 논의 해서 제거 버전이 나오긴 했다.

분산 시스템과 병행하여 코디네이션 시스템 개발은 비효율적이다. 새로 만들기 보다는 주키퍼가 잘 되어 있어서 그대로 쓰기로 한 것이다.

카프카는 클러스터라 하지만 주키퍼는 앙상블이라 한다.

***주키퍼 구성***

분산 시스템의 저장소: 브로커, 토픽 정보 등

리더(write): 로컬에서 처리

팔로워(read): 리더가 처

***카프카 아키텍처***

> 카카오는 5대로 카프카 클러스터를 사용함
> 
> 주키퍼 하나에 카프카 여러 대를 붙여서 사용하기도 함

## 이하 내용

카프카 pub sub 모델은 초기 출시할 때 부터 목적은 decoupling이다.

토픽은 토픽 이름만으로 구분한다. 토픽은 확장할 수 있기에 멀티 컨슈머 멀티 프로듀서를 쓸 수 있다. 원하는대로 확장가능하다.

thickTime: 주기 
initLimit: 초기화 될때 까지의 시간 
syncLimit: 팔로우와 리더가 동기화 하는 시간

## 브로커

카프카라 하면 그 안에 여러대의 브로커가 있는 것이다. 프로듀서가 데이터를 쏘는 것은 카프카가 아니라 브로커에 쏘는 것이다.

프로듀서가 브로커에 쏘면, 컨슈머가 꺼내 가는 흐름이다.

## 파티션

토픽은 내부적으로 병렬 처리를 위해 파티션이라는 것으로 나뉘어져 있고, 파티션은 세그먼트로 나뉘어져 있다.

파티션안에는 메시지들이 저장되는데, 메시지들은 오프셋이라 불리는 자기 번호가 있다.

카프카는 프로듀서가 보낸 메시지 내용은 볼 수 없고 오프셋만 알 수 있다.

## 주키퍼 주소 

서버가 3대일 경우 귀찮아서 kafka 설정에 주키퍼 주소 넣을때 하나만 넣어도 되긴 하지만, 이 경우 한 대에 문제가 생기면 zookeeper는 과반수 법칙에 따라 2대가 살아 있으니 정상으로 작동하지만 kafka에서 문제가 된다. 

## 정상적으로 동작하는지 확인

주키퍼 cli에서 ls / 를 사용해서 우리가 설정한 node가 있는지 확인후, ls /{znode}/brokers/ids 를 확인하면 된다.

좀 더 자세히 보려면 get /{znode}/brokers/ids/{id} 입력해서 결과를 보면 된다.

## Motivation

1. 대량의 실시간 데이터를 다루기 위해 카프카 개발
2. 여러가지 사례들을 통해 연구하고 고민
3. 실시간 로그 통합 같은 이벤트를 처리하기 위해 높은 처리량 필요
4. 빠른 전송 속도 보장
5. 분산, 분할, 실시간 처리를 위해 생겨난 파티셔닝과 컨퓨머 모델
6. 장비의 장애 상황에도 안정적인 유지'

## Persistence

1. 디스크는 느리다라는 선입견
2. linear read와 write는 os에 의해 최적화 되어 있다.
3. os는 최근 적극적으로 디스크 캐싱에 메인 메모리를 사용하다록 하고 있다.
4. 메모리 반환시에는 약간의 성능 저하가 있지만 모든 메모리를 디스크 캐싱으로 전환하려고 한다.
5. JVM에서 JAVA는 객체의 메모리 오버헤드가 높으면 GC를 해주지만, heap이 클수록 불편하고 느리다.
6. 페이지 캐시에 의존하는 것이 유리하다.
7. 운영체제에 있는 것을 사용하면 서비스가 재시작 되어도 캐시가 남아 있다.


## Page Cache(available memory)

1. 시스템 성능을 향상시키기 위해 사용되는 메모리 관리 기법 중 하나.
2. os는 사용하지 않는 부분의 메모리를 페이지 캐시로 유지하여, 페이지 캐시의 컨텐츠에 대해 빠른 액세스를 제공

> 페이지 캐시는 메모리를 물리적으로 전체를 사용한다, 그래서 카프카와 주키퍼를 같은 서버에 올리지 않는 것을 추천한다.

## Dirty Page

1. 페이지 캐시에 있는 데이터를 수정하면, 해당 메모리 페이지는 dirty 상태가 된다.
2. 수정되었지만, 아직 영구 저장소(ex: 하드디스크)에 write 하지 않은 메모리 페이지를 의미
3. 성능적인 부분에서는 효율적이지만, 페이지 캐시와 디스크 내용이 다르면 정합성 문제가 발생할 수도 있다
4. 매번 flush하게 되면, IO 부하가 높아 오히려 성능 저하 발생

## Efficiency

1. 효율성에 상당한 노력을 기울였고, 주목적은 대량의 web activity data를 처리하는 것
2. 다운스트림 인프라 구조에서 애플리케이션 사용에 있어 작은 충돌로 병목이 쉽게 일어난다면, 작은 변화에도 문제가 발생할 수 있음
3. 수십, 수백개의 파이프 라인을 갖는 중앙 집중 클러스터가 사용 패턴에 의해 자주 변경되는 서비스를 지원할 때 매우 중요
4. 비효율적인 disk access를 제외하고 나면, 나머지 비효율성은 2가지 (너무 작은 I/O operation, 과도한 byte copying)
5. 다른 비효율성은 byte copying에서 발생
6. 빈도가 낮을때는 문제가 없지만, 부하가 높으면 문제 발생
7. 브로커와 컨슈머, 프로듀서 사이에 표준화된 메시지 형식 사용
8. 공통된 형식을 유지하여 최적화된 네트워크로 전달 필요
9. 최신의 OS는 페이지 캐시에서 데이터를 소켓으로 전송하기 위해 최적화된 코드 제공
10. 몇몇 경우의 병목현상은 CPU or Disk가 아닌 Network Bandwidth 로 발생
11. 특히 데이터 센터간 대량의 메시지 전송시 발생
12. 카프카의 지원없이 압축할 수 있지만, 압축률은 떨어질 수 있다.
13. 효율적인 압축방식은 각각의 메시지를 압축하는 것보다 다수의 메시지를 압축 - Gzip, Snappy, LZ4, Zstd

> 빠른 전송: snappy < lz4(압축율이 더 좋음)

> 높은 압축율(대량의 데이터를 효과적으로 전송): zstd > gzip - zstd를 표준으로 쓰기 아쉬운 건 지원하지 않는 경우가 많다. gzip 많이 씀

# 토픽

## 디커플링


## 중요한 것

리더가 골고루 분산 되는 것이 정말 중요함

## All Die (서버 전체 죽을 때)

Rack이 3대 있다 가정: 1, 2, 3
1. 1번이 죽으면 2번이 새로운 리더가 된다.
2. 1, 2번이 죽으면 3번이 새로운 리더가 된다.
3. 1, 2, 3번이 전부 죽을 경우 장애 상황이다.

> 이상적인 상태 - 마지막 리더가 다시 살아남
> 1. 운 좋게 3번 서버가 다시 살아나면 메시지 유실 하나 없이 사용 가능하다.
> 2. 그 후 다른 서버도 돌아오면 3번이 리더로 있기에 모든 메시지를 다시 복사한다.

> 현실 - 마지막 리더가 아닌 다른 서버가 살아남
> 1. 최초 리더인 1번이 살아나서 다시 리더가 된다. 새로운 메시지를 받는다
> 2. 마지막에 3번이 리더였는데, 1번이 살아나며 리더가 되니 3번의 메시지가 유실된다.

unclean.leader.election.enable=true : 
완벽하게 신뢰할 수 없는 애를 리더로 할 건지 여부
isr에 포함되어 있지 않아도 리더로 쓸 수 있는지 여부 => 마지막에 살아있던 애가 아니어도 다시 리더가 될 수 있다.
서비스의 정체성이 데이터 유실이 괜찮으면 사용

설정하는 곳 => sudo vi /kafka/config/server.properties

## 리더가 팔로우가 잘 따라오고 있다는 것을 어떻게 알 수 있을까?

리더와 팔로우가 있으면 리더와 팔로우가 복제되었는지 알아야 함

그런데 이러한 오퍼레이션이 시간이 상당히 오래 걸림 - RMQ에서는 이런게 있음

카프카는 대용량처리가 핵심인데 이러한 오퍼레이션이 들어가면 복제하는데 너무 많은 리소스를 낭비하게 됨

그래서 카프카는 복제할 떄 ack를 주고 받지 않음. 그러면 어떻게 받았다는 것을 알 수 있을까?

0번 offset에서 데이터가 있는지 확인하고 있으면 복제해간다.

그다음 1번 offset 데이터가 있는지 확인하고 가져가는데, 이때 리더는 0번 offset 까지는 다 가져간 거라고 생각하게 된다.

여기서 다음 offset 데이터 요청이 안오면, isr에서 비정상 브로커로 보고 제외한다.

## 로그 끝의 일부는 리더와 팔로워가 일치하지 않을 수 있다.

위의 내용 이어서 ack를 주고받아서 정밀하게 하는게 아니기 때문에 일치하지 않을 수 있다.

이런 문제 때문에 쓰기는 리더만 하게 만든 것 같다.

## 종료 방식

**예기치 못한 종료**

**제어된 종료(graceful stop)**

# 프로듀서

bootstrap.servers => 카프카 서버
buffer memory => producer에서 배치를 가지고 있는 위치 => 조금 크페 잡아줘야 한다
compression.type => 압축파일 타입
retries => 재실행 횟수, 따로 관리 가능
batch.size => 배치 사이즈
linger.ms => 배치를 할 때 타이머가 동작하는데 이 시간만큼 대기함 - 배치 처리할거면 값을 조금 크게
max.in.flight.requests.per.connection => 한번 커넥션할 때 ack를 보내지 않고 몇개의 메시지를 보낼 수 있을 것인가
key.serializer
value.serializer

> 버퍼 메모리가 배치 사이즈보다 훨씬 커야 한다.
> key에는 string, value에는 json으로 쓰는 경우가 있어서 serializer가 따로 있음

# 파티셔너

key, value 형태로 메시지를 전송함 - key는 optional

순차적으로 데이터를 받아야 하는 경우 key를 사용함

쇼핑몰 같은 경우 고객 단위별로 메시지를 순차적으로 처리해야 할 경우 key를 사용함

사용은 비추 
- 파티션의 개수가 변경 될 경우 처음 설계 방식에 혼선이 발생함
- hot partition 발생할 수 있음 => 쏠림 현상 생길 수 있음

**defaultPartitioner**

2.4 이전 버전은 round robin이었는데, 이후에는 sticky로 변경 되었다.

3개의 메시지가 배치에 들어오면 전송 된다고 할 때

round robin : 차례대로 하나씩 넣어서 오래 걸림

sticky : 하나의 배치에 3개씩 넣고 먼저 보내 버려서 빠름

**acks**

acks = 0 : 매우 빠르게 전송할 수 있지만 파티션의 리더가 받았는지 알 수 없음 - at most once (0.29ms)

acks = 1 : 메시지 전송도 빠른 편이고, 파티션 리더가 받았는지 확인 (가장 많이 사용하고, 기본 값) (1.05ms)

acks = ALL : 메시지 전송은 느리지만, 손실 없는 메시지 전송 가능 (2.05ms)

# 2주차 부터 다시 정리해야 함 - 중요한  것만 적었음
